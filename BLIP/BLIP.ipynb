{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1Knxl6Ix2OCOGSHuYJ9QHGAc8kv_z8kdH",
      "authorship_tag": "ABX9TyNSjzKbJx7Zff9JcLLtrqS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a815bd77807437b857a8badd96df472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c3bda07257d499ea7b2fe33905a6ebc",
              "IPY_MODEL_57a64db038a542d3b767ae1f2b7f1b22",
              "IPY_MODEL_c5b6dfcb8ee146f596a526919ddc9584"
            ],
            "layout": "IPY_MODEL_b791f01bdd5e4fc6a2c48a254d39ba8d"
          }
        },
        "6c3bda07257d499ea7b2fe33905a6ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_400eb3d8819547bebcc8d2b90dccc0d7",
            "placeholder": "​",
            "style": "IPY_MODEL_7dd9121b3c64400b9ef6ec4d63502266",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "57a64db038a542d3b767ae1f2b7f1b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3813f985c8924750aaecd5888f1009df",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40c506148b2641f5ad8ef832d0587516",
            "value": 8
          }
        },
        "c5b6dfcb8ee146f596a526919ddc9584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9358d8c89e34f73b6158c74d3fd837f",
            "placeholder": "​",
            "style": "IPY_MODEL_ec290299fdad404892a750be7aa3b50f",
            "value": " 8/8 [00:02&lt;00:00,  3.27it/s]"
          }
        },
        "b791f01bdd5e4fc6a2c48a254d39ba8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "400eb3d8819547bebcc8d2b90dccc0d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dd9121b3c64400b9ef6ec4d63502266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3813f985c8924750aaecd5888f1009df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c506148b2641f5ad8ef832d0587516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9358d8c89e34f73b6158c74d3fd837f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec290299fdad404892a750be7aa3b50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vignesh-397/Image_Captioning/blob/main/BLIP/BLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wu1BNLoNVbtp",
        "outputId": "7ef358c8-dd79-4424-c96e-64f026212f79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.50.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "cd4f5beb52dd41c4b4c868fb8b168816"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GT3MHAvfVMyJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from datasets import Dataset as HFDataset\n",
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "captions_file = \"/content/drive/MyDrive/Major Project 2024/Dataset/Custom_Dataset/Image Captioning Data.xlsx\"\n",
        "image_folder = \"/content/drive/MyDrive/Major Project 2024/Dataset/Custom_Dataset/Images\""
      ],
      "metadata": {
        "id": "k-zs_ZX7VSNc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(captions_file)\n",
        "\n",
        "# Convert captions to a dictionary\n",
        "image_captions = {}\n",
        "for _, row in df.iterrows():\n",
        "    image_captions[row[\"Name\"]] = [row[f\"Caption-{i}\"] for i in range(1, 6)]"
      ],
      "metadata": {
        "id": "YfS4CmpbVjbA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image transformations\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "hh0GnUd0Vj87"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, image_folder, captions, processor):\n",
        "        self.image_folder = image_folder\n",
        "        self.captions = captions\n",
        "        self.processor = processor\n",
        "        self.image_filenames = list(self.captions.keys())\n",
        "        #self.image_filenames = [int(os.path.splitext(filename)[0]) for filename in self.image_filenames] #change 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_filenames[idx]\n",
        "        # Format image name to include leading zeros and .jpg extension\n",
        "        image_name_formatted = f\"{int(image_name):03d}.jpg\"\n",
        "        image_path = os.path.join(self.image_folder, image_name_formatted)\n",
        "\n",
        "        # Load and process the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = image_transform(image)\n",
        "\n",
        "        # Select a random caption\n",
        "        # Use the original image name (image_name) to access captions,\n",
        "        # ensuring consistency with the keys in the self.captions dictionary\n",
        "        caption = self.captions[image_name][torch.randint(0, 5, (1,)).item()]\n",
        "\n",
        "        encoding = self.processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        encoding[\"text\"] = caption\n",
        "\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "yca3ytOqXOsW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    processed_batch = {}\n",
        "    for key in batch[0].keys():\n",
        "        if key != \"text\":\n",
        "            processed_batch[key] = torch.stack([example[key] for example in batch])\n",
        "        else:\n",
        "            # Ensure that the text input is a list of strings\n",
        "            text_inputs = processor.tokenizer(\n",
        "                [str(example[\"text\"]) for example in batch],  # Convert to string\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
        "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
        "    return processed_batch"
      ],
      "metadata": {
        "id": "4K2mKUKiVmr-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BLIP-2 model and processor\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"ybelkada/blip2-opt-2.7b-fp16-sharded\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6a815bd77807437b857a8badd96df472",
            "6c3bda07257d499ea7b2fe33905a6ebc",
            "57a64db038a542d3b767ae1f2b7f1b22",
            "c5b6dfcb8ee146f596a526919ddc9584",
            "b791f01bdd5e4fc6a2c48a254d39ba8d",
            "400eb3d8819547bebcc8d2b90dccc0d7",
            "7dd9121b3c64400b9ef6ec4d63502266",
            "3813f985c8924750aaecd5888f1009df",
            "40c506148b2641f5ad8ef832d0587516",
            "f9358d8c89e34f73b6158c74d3fd837f",
            "ec290299fdad404892a750be7aa3b50f"
          ]
        },
        "id": "WJa9TOdGVrvE",
        "outputId": "0e0cb06d-5435-4e86-f195-83aab5d6b129"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a815bd77807437b857a8badd96df472"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKFKsOYCVu5V",
        "outputId": "73ebccf9-391a-4c6a-df6a-aaba588590ac"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 5,242,880 || all params: 3,749,922,816 || trainable%: 0.1398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_dataset = ImageCaptioningDataset(image_folder, image_captions, processor)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=3, collate_fn=collate_fn)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n"
      ],
      "metadata": {
        "id": "WaFoptCIV5wK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training Loop\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jhNo8rGBV7SU",
        "outputId": "7ff33f4e-b656-493e-cf9f-de376892c191"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Blip2ForConditionalGeneration(\n",
              "      (vision_model): Blip2VisionModel(\n",
              "        (embeddings): Blip2VisionEmbeddings(\n",
              "          (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
              "        )\n",
              "        (encoder): Blip2Encoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-38): 39 x Blip2EncoderLayer(\n",
              "              (self_attn): Blip2Attention(\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (qkv): Linear8bitLt(in_features=1408, out_features=4224, bias=True)\n",
              "                (projection): Linear8bitLt(in_features=1408, out_features=1408, bias=True)\n",
              "              )\n",
              "              (layer_norm1): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Blip2MLP(\n",
              "                (activation_fn): GELUActivation()\n",
              "                (fc1): Linear8bitLt(in_features=1408, out_features=6144, bias=True)\n",
              "                (fc2): Linear8bitLt(in_features=6144, out_features=1408, bias=True)\n",
              "              )\n",
              "              (layer_norm2): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (post_layernorm): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (qformer): Blip2QFormerModel(\n",
              "        (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (encoder): Blip2QFormerEncoder(\n",
              "          (layer): ModuleList(\n",
              "            (0): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (crossattention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (1): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (2): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (crossattention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (3): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (4): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (crossattention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (5): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (6): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (crossattention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (7): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (8): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (crossattention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (9): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (10): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (crossattention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (11): Blip2QFormerLayer(\n",
              "              (attention): Blip2QFormerAttention(\n",
              "                (attention): Blip2QFormerMultiHeadAttention(\n",
              "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): Blip2QFormerSelfOutput(\n",
              "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate_query): Blip2QFormerIntermediate(\n",
              "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output_query): Blip2QFormerOutput(\n",
              "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (language_projection): Linear8bitLt(in_features=768, out_features=2560, bias=True)\n",
              "      (language_model): OPTForCausalLM(\n",
              "        (model): OPTModel(\n",
              "          (decoder): OPTDecoder(\n",
              "            (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n",
              "            (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
              "            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "            (layers): ModuleList(\n",
              "              (0-31): 32 x OPTDecoderLayer(\n",
              "                (self_attn): OPTSdpaAttention(\n",
              "                  (k_proj): lora.Linear8bitLt(\n",
              "                    (base_layer): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "                  (q_proj): lora.Linear8bitLt(\n",
              "                    (base_layer): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (out_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "                )\n",
              "                (activation_fn): ReLU()\n",
              "                (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "                (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
              "                (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
              "                (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):  # Adjust epochs as needed\n",
        "    print(f\"Epoch {epoch+1}:\")\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch.pop(\"input_ids\").to(device)\n",
        "        # Change the dtype to torch.float32 when moving to device\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device, torch.float32) # Changed to float32\n",
        "\n",
        "        # Cast the model to float32 before the forward pass\n",
        "        with torch.autocast(device_type=device, dtype=torch.float32): # New line to force float32\n",
        "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        print(f\"Batch {idx+1}, Loss: {loss.item()}\")\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CIY4GEubV_ew",
        "outputId": "736f9b9d-c82e-4c24-ec7b-34b91334049f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1, Loss: 3.2914822101593018\n",
            "Batch 2, Loss: 2.8990631103515625\n",
            "Batch 3, Loss: 2.9845874309539795\n",
            "Batch 4, Loss: 2.824375867843628\n",
            "Batch 5, Loss: 2.1578121185302734\n",
            "Batch 6, Loss: 2.5487258434295654\n",
            "Batch 7, Loss: 2.501188039779663\n",
            "Batch 8, Loss: 2.3063182830810547\n",
            "Batch 9, Loss: 3.972632884979248\n",
            "Batch 10, Loss: 2.7438015937805176\n",
            "Batch 11, Loss: 2.2262442111968994\n",
            "Batch 12, Loss: 2.807004451751709\n",
            "Batch 13, Loss: 2.2125654220581055\n",
            "Batch 14, Loss: 2.403764009475708\n",
            "Batch 15, Loss: 3.4833078384399414\n",
            "Batch 16, Loss: 2.9445085525512695\n",
            "Batch 17, Loss: 4.217895984649658\n",
            "Batch 18, Loss: 1.90729558467865\n",
            "Batch 19, Loss: 1.803754210472107\n",
            "Batch 20, Loss: 2.274237871170044\n",
            "Batch 21, Loss: 3.139176368713379\n",
            "Batch 22, Loss: 2.4953346252441406\n",
            "Batch 23, Loss: 2.0718533992767334\n",
            "Batch 24, Loss: 1.9580312967300415\n",
            "Batch 25, Loss: 2.3912527561187744\n",
            "Batch 26, Loss: 2.333970546722412\n",
            "Batch 27, Loss: 2.3069827556610107\n",
            "Batch 28, Loss: 3.240786552429199\n",
            "Batch 29, Loss: 2.140280246734619\n",
            "Batch 30, Loss: 2.0574798583984375\n",
            "Batch 31, Loss: 2.582953691482544\n",
            "Batch 32, Loss: 3.333801031112671\n",
            "Batch 33, Loss: 1.8771469593048096\n",
            "Batch 34, Loss: 2.296064853668213\n",
            "Batch 35, Loss: 2.5160415172576904\n",
            "Batch 36, Loss: 2.494466781616211\n",
            "Batch 37, Loss: 1.912501573562622\n",
            "Batch 38, Loss: 1.9518616199493408\n",
            "Batch 39, Loss: 2.403013229370117\n",
            "Batch 40, Loss: 2.6348841190338135\n",
            "Batch 41, Loss: 1.9577969312667847\n",
            "Batch 42, Loss: 2.5516440868377686\n",
            "Batch 43, Loss: 1.6561795473098755\n",
            "Batch 44, Loss: 2.713132619857788\n",
            "Batch 45, Loss: 2.803739309310913\n",
            "Batch 46, Loss: 3.2993266582489014\n",
            "Batch 47, Loss: 2.0504140853881836\n",
            "Batch 48, Loss: 2.814741849899292\n",
            "Batch 49, Loss: 2.506666421890259\n",
            "Batch 50, Loss: 2.612342596054077\n",
            "Batch 51, Loss: 2.4222571849823\n",
            "Batch 52, Loss: 2.1286277770996094\n",
            "Batch 53, Loss: 2.0763983726501465\n",
            "Batch 54, Loss: 1.7780356407165527\n",
            "Batch 55, Loss: 3.2087769508361816\n",
            "Batch 56, Loss: 2.3829257488250732\n",
            "Batch 57, Loss: 2.6737773418426514\n",
            "Batch 58, Loss: 2.2244558334350586\n",
            "Batch 59, Loss: 2.771563768386841\n",
            "Batch 60, Loss: 1.8013174533843994\n",
            "Batch 61, Loss: 1.765843391418457\n",
            "Batch 62, Loss: 2.654198408126831\n",
            "Batch 63, Loss: 2.875408411026001\n",
            "Batch 64, Loss: 2.6119368076324463\n",
            "Batch 65, Loss: 2.386455535888672\n",
            "Batch 66, Loss: 2.227527618408203\n",
            "Batch 67, Loss: 2.7555088996887207\n",
            "Batch 68, Loss: 2.500962972640991\n",
            "Batch 69, Loss: 1.9006154537200928\n",
            "Batch 70, Loss: 2.775784492492676\n",
            "Batch 71, Loss: 3.0002787113189697\n",
            "Batch 72, Loss: 4.089201927185059\n",
            "Batch 73, Loss: 2.4892826080322266\n",
            "Batch 74, Loss: 2.97214937210083\n",
            "Batch 75, Loss: 2.386157751083374\n",
            "Batch 76, Loss: 2.425631284713745\n",
            "Batch 77, Loss: 2.166266918182373\n",
            "Batch 78, Loss: 2.218841791152954\n",
            "Batch 79, Loss: 2.5916099548339844\n",
            "Batch 80, Loss: 1.846603512763977\n",
            "Batch 81, Loss: 2.5379831790924072\n",
            "Batch 82, Loss: 2.9088637828826904\n",
            "Batch 83, Loss: 2.2142605781555176\n",
            "Batch 84, Loss: 1.9001766443252563\n",
            "Batch 85, Loss: 3.3268744945526123\n",
            "Batch 86, Loss: 1.7376365661621094\n",
            "Batch 87, Loss: 2.392519474029541\n",
            "Batch 88, Loss: 2.778329610824585\n",
            "Batch 89, Loss: 2.4929025173187256\n",
            "Batch 90, Loss: 2.010632038116455\n",
            "Batch 91, Loss: 2.1805403232574463\n",
            "Batch 92, Loss: 1.9067076444625854\n",
            "Batch 93, Loss: 2.4752142429351807\n",
            "Batch 94, Loss: 2.7672977447509766\n",
            "Batch 95, Loss: 1.8872438669204712\n",
            "Batch 96, Loss: 2.5346527099609375\n",
            "Batch 97, Loss: 2.474428176879883\n",
            "Batch 98, Loss: 2.2756526470184326\n",
            "Batch 99, Loss: 2.4681215286254883\n",
            "Batch 100, Loss: 2.400090456008911\n",
            "Batch 101, Loss: 2.189887523651123\n",
            "Batch 102, Loss: 2.498629093170166\n",
            "Batch 103, Loss: 2.2134292125701904\n",
            "Batch 104, Loss: 3.743907928466797\n",
            "Batch 105, Loss: 3.409316062927246\n",
            "Batch 106, Loss: 3.4507176876068115\n",
            "Batch 107, Loss: 2.543440818786621\n",
            "Batch 108, Loss: 2.5156784057617188\n",
            "Batch 109, Loss: 1.7777572870254517\n",
            "Batch 110, Loss: 2.5258142948150635\n",
            "Epoch 2:\n",
            "Batch 1, Loss: 2.193469762802124\n",
            "Batch 2, Loss: 2.134155511856079\n",
            "Batch 3, Loss: 2.593471050262451\n",
            "Batch 4, Loss: 3.0676214694976807\n",
            "Batch 5, Loss: 1.8250861167907715\n",
            "Batch 6, Loss: 2.5315182209014893\n",
            "Batch 7, Loss: 2.071894407272339\n",
            "Batch 8, Loss: 2.9219400882720947\n",
            "Batch 9, Loss: 1.8684202432632446\n",
            "Batch 10, Loss: 2.3238797187805176\n",
            "Batch 11, Loss: 2.6602041721343994\n",
            "Batch 12, Loss: 2.6030638217926025\n",
            "Batch 13, Loss: 2.452094554901123\n",
            "Batch 14, Loss: 2.219252824783325\n",
            "Batch 15, Loss: 1.7656301259994507\n",
            "Batch 16, Loss: 2.523984670639038\n",
            "Batch 17, Loss: 2.251039981842041\n",
            "Batch 18, Loss: 1.9495227336883545\n",
            "Batch 19, Loss: 2.3824055194854736\n",
            "Batch 20, Loss: 1.7767237424850464\n",
            "Batch 21, Loss: 2.111997127532959\n",
            "Batch 22, Loss: 2.8955929279327393\n",
            "Batch 23, Loss: 2.049151659011841\n",
            "Batch 24, Loss: 1.7523272037506104\n",
            "Batch 25, Loss: 2.4723703861236572\n",
            "Batch 26, Loss: 2.8994944095611572\n",
            "Batch 27, Loss: 2.1102135181427\n",
            "Batch 28, Loss: 2.782991886138916\n",
            "Batch 29, Loss: 2.485166311264038\n",
            "Batch 30, Loss: 1.958949327468872\n",
            "Batch 31, Loss: 2.8531572818756104\n",
            "Batch 32, Loss: 1.7718168497085571\n",
            "Batch 33, Loss: 1.8571118116378784\n",
            "Batch 34, Loss: 2.4329001903533936\n",
            "Batch 35, Loss: 1.6824623346328735\n",
            "Batch 36, Loss: 3.4599037170410156\n",
            "Batch 37, Loss: 1.657073974609375\n",
            "Batch 38, Loss: 1.7240512371063232\n",
            "Batch 39, Loss: 2.224513292312622\n",
            "Batch 40, Loss: 2.5179367065429688\n",
            "Batch 41, Loss: 2.05937123298645\n",
            "Batch 42, Loss: 2.5006067752838135\n",
            "Batch 43, Loss: 2.5169873237609863\n",
            "Batch 44, Loss: 1.6420187950134277\n",
            "Batch 45, Loss: 2.701734781265259\n",
            "Batch 46, Loss: 2.0364434719085693\n",
            "Batch 47, Loss: 2.3772976398468018\n",
            "Batch 48, Loss: 1.952432632446289\n",
            "Batch 49, Loss: 2.2762351036071777\n",
            "Batch 50, Loss: 3.427762031555176\n",
            "Batch 51, Loss: 1.4927295446395874\n",
            "Batch 52, Loss: 2.124901533126831\n",
            "Batch 53, Loss: 2.3180737495422363\n",
            "Batch 54, Loss: 2.884605884552002\n",
            "Batch 55, Loss: 2.2617220878601074\n",
            "Batch 56, Loss: 2.3718879222869873\n",
            "Batch 57, Loss: 1.544562578201294\n",
            "Batch 58, Loss: 2.9035017490386963\n",
            "Batch 59, Loss: 2.703101634979248\n",
            "Batch 60, Loss: 2.1241605281829834\n",
            "Batch 61, Loss: 2.8122189044952393\n",
            "Batch 62, Loss: 2.6925666332244873\n",
            "Batch 63, Loss: 1.7349300384521484\n",
            "Batch 64, Loss: 2.731621503829956\n",
            "Batch 65, Loss: 3.1977081298828125\n",
            "Batch 66, Loss: 2.258552074432373\n",
            "Batch 67, Loss: 1.9891486167907715\n",
            "Batch 68, Loss: 2.6062028408050537\n",
            "Batch 69, Loss: 1.6210521459579468\n",
            "Batch 70, Loss: 2.080518960952759\n",
            "Batch 71, Loss: 1.8837566375732422\n",
            "Batch 72, Loss: 1.6747878789901733\n",
            "Batch 73, Loss: 1.8184733390808105\n",
            "Batch 74, Loss: 2.2503302097320557\n",
            "Batch 75, Loss: 2.4007503986358643\n",
            "Batch 76, Loss: 2.0014612674713135\n",
            "Batch 77, Loss: 1.8239370584487915\n",
            "Batch 78, Loss: 2.399810314178467\n",
            "Batch 79, Loss: 2.8028838634490967\n",
            "Batch 80, Loss: 2.3733088970184326\n",
            "Batch 81, Loss: 2.416255235671997\n",
            "Batch 82, Loss: 2.495452642440796\n",
            "Batch 83, Loss: 2.370121955871582\n",
            "Batch 84, Loss: 2.831845998764038\n",
            "Batch 85, Loss: 2.3126699924468994\n",
            "Batch 86, Loss: 2.063999652862549\n",
            "Batch 87, Loss: 2.0601108074188232\n",
            "Batch 88, Loss: 2.2396609783172607\n",
            "Batch 89, Loss: 2.130373001098633\n",
            "Batch 90, Loss: 1.6984645128250122\n",
            "Batch 91, Loss: 2.175356149673462\n",
            "Batch 92, Loss: 1.9008440971374512\n",
            "Batch 93, Loss: 1.966802716255188\n",
            "Batch 94, Loss: 2.7754387855529785\n",
            "Batch 95, Loss: 1.9285532236099243\n",
            "Batch 96, Loss: 2.556187629699707\n",
            "Batch 97, Loss: 2.2350144386291504\n",
            "Batch 98, Loss: 1.4078514575958252\n",
            "Batch 99, Loss: 2.0495731830596924\n",
            "Batch 100, Loss: 1.8570724725723267\n",
            "Batch 101, Loss: 1.9709343910217285\n",
            "Batch 102, Loss: 1.7427033185958862\n",
            "Batch 103, Loss: 1.8997033834457397\n",
            "Batch 104, Loss: 1.9525136947631836\n",
            "Batch 105, Loss: 1.9297633171081543\n",
            "Batch 106, Loss: 1.827255368232727\n",
            "Batch 107, Loss: 2.6633665561676025\n",
            "Batch 108, Loss: 1.6447969675064087\n",
            "Batch 109, Loss: 1.4495419263839722\n",
            "Batch 110, Loss: 3.014944076538086\n",
            "Epoch 3:\n",
            "Batch 1, Loss: 2.4622673988342285\n",
            "Batch 2, Loss: 1.84543776512146\n",
            "Batch 3, Loss: 1.7477480173110962\n",
            "Batch 4, Loss: 2.0259933471679688\n",
            "Batch 5, Loss: 2.4741463661193848\n",
            "Batch 6, Loss: 2.1316444873809814\n",
            "Batch 7, Loss: 2.362253427505493\n",
            "Batch 8, Loss: 1.8411648273468018\n",
            "Batch 9, Loss: 2.703009605407715\n",
            "Batch 10, Loss: 2.1799964904785156\n",
            "Batch 11, Loss: 1.598891258239746\n",
            "Batch 12, Loss: 1.8356612920761108\n",
            "Batch 13, Loss: 2.0100817680358887\n",
            "Batch 14, Loss: 2.3589670658111572\n",
            "Batch 15, Loss: 1.8242385387420654\n",
            "Batch 16, Loss: 3.363541841506958\n",
            "Batch 17, Loss: 2.6907408237457275\n",
            "Batch 18, Loss: 2.3266780376434326\n",
            "Batch 19, Loss: 2.3915319442749023\n",
            "Batch 20, Loss: 1.350414752960205\n",
            "Batch 21, Loss: 3.0427236557006836\n",
            "Batch 22, Loss: 1.6500693559646606\n",
            "Batch 23, Loss: 1.7949950695037842\n",
            "Batch 24, Loss: 2.1820104122161865\n",
            "Batch 25, Loss: 2.7612218856811523\n",
            "Batch 26, Loss: 2.962641477584839\n",
            "Batch 27, Loss: 2.405669927597046\n",
            "Batch 28, Loss: 1.930637240409851\n",
            "Batch 29, Loss: 1.866774320602417\n",
            "Batch 30, Loss: 1.9717494249343872\n",
            "Batch 31, Loss: 1.7690141201019287\n",
            "Batch 32, Loss: 1.7156050205230713\n",
            "Batch 33, Loss: 2.2325313091278076\n",
            "Batch 34, Loss: 1.924371361732483\n",
            "Batch 35, Loss: 1.6513005495071411\n",
            "Batch 36, Loss: 1.8640447854995728\n",
            "Batch 37, Loss: 1.590935230255127\n",
            "Batch 38, Loss: 3.483217716217041\n",
            "Batch 39, Loss: 2.08670973777771\n",
            "Batch 40, Loss: 2.484346389770508\n",
            "Batch 41, Loss: 1.9083993434906006\n",
            "Batch 42, Loss: 2.4221174716949463\n",
            "Batch 43, Loss: 1.6188026666641235\n",
            "Batch 44, Loss: 1.913398265838623\n",
            "Batch 45, Loss: 1.973933219909668\n",
            "Batch 46, Loss: 3.676534652709961\n",
            "Batch 47, Loss: 1.6462944746017456\n",
            "Batch 48, Loss: 1.8293890953063965\n",
            "Batch 49, Loss: 1.4225407838821411\n",
            "Batch 50, Loss: 2.5156283378601074\n",
            "Batch 51, Loss: 1.4889870882034302\n",
            "Batch 52, Loss: 2.242178440093994\n",
            "Batch 53, Loss: 2.3396995067596436\n",
            "Batch 54, Loss: 1.9608581066131592\n",
            "Batch 55, Loss: 2.016695022583008\n",
            "Batch 56, Loss: 2.3458590507507324\n",
            "Batch 57, Loss: 2.689401388168335\n",
            "Batch 58, Loss: 2.21370267868042\n",
            "Batch 59, Loss: 1.830407738685608\n",
            "Batch 60, Loss: 2.9745538234710693\n",
            "Batch 61, Loss: 1.945915699005127\n",
            "Batch 62, Loss: 2.2175450325012207\n",
            "Batch 63, Loss: 2.0160412788391113\n",
            "Batch 64, Loss: 1.6555675268173218\n",
            "Batch 65, Loss: 2.3383145332336426\n",
            "Batch 66, Loss: 2.0562098026275635\n",
            "Batch 67, Loss: 2.305185079574585\n",
            "Batch 68, Loss: 2.0262937545776367\n",
            "Batch 69, Loss: 1.566348671913147\n",
            "Batch 70, Loss: 2.073341131210327\n",
            "Batch 71, Loss: 1.271652102470398\n",
            "Batch 72, Loss: 2.400508165359497\n",
            "Batch 73, Loss: 1.667143702507019\n",
            "Batch 74, Loss: 2.469618082046509\n",
            "Batch 75, Loss: 2.447657823562622\n",
            "Batch 76, Loss: 2.996375799179077\n",
            "Batch 77, Loss: 2.3855390548706055\n",
            "Batch 78, Loss: 2.106555700302124\n",
            "Batch 79, Loss: 1.9486545324325562\n",
            "Batch 80, Loss: 2.3570759296417236\n",
            "Batch 81, Loss: 2.1839704513549805\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-eeae0cc213d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Adjust epochs as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Change the dtype to torch.float32 when moving to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-ed0af5a94909>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Load and process the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m                         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodermaxblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                                 \u001b[0;31m# truncated png/gif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36mload_read\u001b[0;34m(self, read_bytes)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mso\u001b[0m \u001b[0mlibjpeg\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mfinish\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \"\"\"\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOAD_TRUNCATED_IMAGES\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_ended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}